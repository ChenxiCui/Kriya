## config file for the Kriya training ##

# Kriya model root (The pre-processed files and Kriya models would be created under this)
KRIYA_MODELS=<model-dir>

# Kriya source-code (Where to find the phrase-extractor and decoder)
KRIYA_PXTR=<Kriya Phrase-extractor top-level directory>
KRIYA_DEC=<Kriya decoder top-level directory>

# Moses scripts rootdir (required for SCFG rule extraction) and MERT binary dir (for creating MERT script)
MOSES_SCRIPTS=/cs/natlang-sw/Linux-x86_64/NL/MT/MOSES/SVN_20101028/scripts
MERT_BIN=/cs/natlang-sw/Linux-x86_64/NL/MT/MOSES/SVN_20101028/bin

# Language and training parameters
src=fr
tgt=en
max_phr_len=10

## Corpus information
## {TRAIN/DEV/TEST}_PRE take a ',' separated list of the prefixes (excluding the language codes)
# Training set info
TRAIN_DIR=/cs/natlang-data/wmt11/ensemble/en-fr/training
TRAIN_PRE=

# Development set info
DEV_DIR=/cs/natlang-data/wmt11/ensemble/en-fr/dev
DEV_PRE=

# Test set info
TEST_DIR=/cs/natlang-data/wmt11/ensemble/en-fr/test
TEST_PRE=

## Pre-processing steps and parameters
## clean: Corpus cleaning (remove blank lines and setences longer than max_sent_len); if True this will be applied for both src and tgt
clean=True
max_sent_len=80
## Other pre-processing steps are defined separately for source and target and take ',' separate values
## "lc": Lowercase the corpus and "tok": Tokenize the corpus
src_pp=lc,tok
tgt_pp=lc,tok

# Moses phrase extraction
giza_parts=5
first_step=1
last_step=5

# Kriya model training options
## fr_side_len: Controls the number of terminals and non-terminals in the source side (default 5)
## Setting this to 7 makes the model comparable to that of Moses trained with max_phr_len 7 (at the cost of a larger model)
## non_terminals: Kriya can be trained with just 1 non-terminal (default 2)
fr_side_len=5
non_terminals=2
split_size=10000
sent_per_file=200

# Additional qsub params
qsub_notify="-m a -M <email-addr>"

## Kriya decoder config info (for creating config file for tuning)
shallow_hiero=False
use_srilm=False
cbp=500
nbest_size=100
nbest_format=True
lm_file=/cs/natlang-projects/data/lm/en/gigaword.5gram.lm.en.v4.kenlm
lm_order=5

## Options for tuning step: currently supports 'mert' and 'pro' for optimizer
## MERT uses the line search optimizer from MOSES
## PRO uses in-built pairwise ranking (requires MegaM/ SVMRank) and supports metrics other than BLEU
optimizer=pro
opt_metric=bleu
opt_sampler=rank

# Which MERT? This will automatically create script for running MosesMERT or zMERT
# Automatic script creation for zmert will soon be supported (ignored if optimizer option is set to 'pro')
which_mert=mosesmert

